{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7880750,"sourceType":"datasetVersion","datasetId":4625365},{"sourceId":8009007,"sourceType":"datasetVersion","datasetId":4717292},{"sourceId":8860524,"sourceType":"datasetVersion","datasetId":5334482},{"sourceId":8860538,"sourceType":"datasetVersion","datasetId":5334491},{"sourceId":9381856,"sourceType":"datasetVersion","datasetId":5691656},{"sourceId":9417760,"sourceType":"datasetVersion","datasetId":5719762},{"sourceId":9419954,"sourceType":"datasetVersion","datasetId":5721394},{"sourceId":9585928,"sourceType":"datasetVersion","datasetId":5845787},{"sourceId":9586037,"sourceType":"datasetVersion","datasetId":5845862},{"sourceId":9593785,"sourceType":"datasetVersion","datasetId":5851860},{"sourceId":9606542,"sourceType":"datasetVersion","datasetId":5861277},{"sourceId":9606618,"sourceType":"datasetVersion","datasetId":5861337},{"sourceId":9911682,"sourceType":"datasetVersion","datasetId":6090166}],"dockerImageVersionId":30665,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-16T02:26:29.587084Z","iopub.execute_input":"2024-11-16T02:26:29.58745Z","iopub.status.idle":"2024-11-16T02:26:30.055622Z","shell.execute_reply.started":"2024-11-16T02:26:29.587415Z","shell.execute_reply":"2024-11-16T02:26:30.054725Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Read csv","metadata":{}},{"cell_type":"code","source":"# df=pd.read_csv('/kaggle/input/subject3/10003_26038.csv')\n# df=pd.read_csv('/kaggle/input/subject-5/10015_10063.csv')\n# df=pd.read_csv('/kaggle/input/subject-9/12808_17752.csv')\n# df=pd.read_csv('/kaggle/input/testannotation/10000_17728.csv')\ndf=pd.read_csv('/kaggle/input/9955-18184/9955_18184.csv')\n# df=pd.read_csv('/kaggle/input/9931-17578/9931_17578.csv')\n# df=pd.read_csv('/kaggle/input/subject-5/10015_10063.csv')\n\n\nprint(df.isna().sum())\ndf_og = df","metadata":{"execution":{"iopub.status.busy":"2024-11-16T02:26:33.630456Z","iopub.execute_input":"2024-11-16T02:26:33.630982Z","iopub.status.idle":"2024-11-16T02:28:15.720934Z","shell.execute_reply.started":"2024-11-16T02:26:33.630946Z","shell.execute_reply":"2024-11-16T02:28:15.719845Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"n = len(df)\nn","metadata":{"execution":{"iopub.status.busy":"2024-11-16T02:28:32.29019Z","iopub.execute_input":"2024-11-16T02:28:32.290915Z","iopub.status.idle":"2024-11-16T02:28:32.297413Z","shell.execute_reply.started":"2024-11-16T02:28:32.290881Z","shell.execute_reply":"2024-11-16T02:28:32.296593Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## remove rows with \"sleep stage ?\"","metadata":{}},{"cell_type":"code","source":"df = df_og","metadata":{"execution":{"iopub.status.busy":"2024-10-12T07:19:05.998Z","iopub.execute_input":"2024-10-12T07:19:05.998343Z","iopub.status.idle":"2024-10-12T07:19:06.002393Z","shell.execute_reply.started":"2024-10-12T07:19:05.998314Z","shell.execute_reply":"2024-10-12T07:19:06.001608Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# n = len(df)\ndf = df.iloc[0:8033528]\nlen(df)","metadata":{"execution":{"iopub.status.busy":"2024-10-28T10:21:32.835266Z","iopub.execute_input":"2024-10-28T10:21:32.836072Z","iopub.status.idle":"2024-10-28T10:21:32.84207Z","shell.execute_reply.started":"2024-10-28T10:21:32.836038Z","shell.execute_reply":"2024-10-28T10:21:32.841142Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.columns","metadata":{"execution":{"iopub.status.busy":"2024-11-01T05:13:43.600736Z","iopub.execute_input":"2024-11-01T05:13:43.601663Z","iopub.status.idle":"2024-11-01T05:13:43.608902Z","shell.execute_reply.started":"2024-11-01T05:13:43.601623Z","shell.execute_reply":"2024-11-01T05:13:43.60789Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(df.isna().sum())","metadata":{"execution":{"iopub.status.busy":"2024-11-01T05:13:44.414927Z","iopub.execute_input":"2024-11-01T05:13:44.415821Z","iopub.status.idle":"2024-11-01T05:13:44.701517Z","shell.execute_reply.started":"2024-11-01T05:13:44.415778Z","shell.execute_reply":"2024-11-01T05:13:44.700503Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\n\n# Print maximum values of all columns\nprint(\"Maximum values of all columns:\")\nprint(df.max())\n\n# Print minimum values of all columns\nprint(\"\\nMinimum values of all columns:\")\nprint(df.min())\n","metadata":{"execution":{"iopub.status.busy":"2024-11-02T05:28:21.358926Z","iopub.execute_input":"2024-11-02T05:28:21.359725Z","iopub.status.idle":"2024-11-02T05:28:22.515238Z","shell.execute_reply.started":"2024-11-02T05:28:21.359688Z","shell.execute_reply":"2024-11-02T05:28:22.51416Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Step 1: Create a balanced dataset\n# anomaly_rows = df[df['anomalies'] == 1].sample(10000, random_state=42)\n# non_anomaly_rows = df[df['anomalies'] == 0].sample(10000, random_state=42)\n\n# anomaly_rows = wavelet_features_df[wavelet_features_df['anomalies'] == 1].sample(10000, random_state=42)\n# non_anomaly_rows = wavelet_features_df[wavelet_features_df['anomalies'] == 0].sample(10000, random_state=42)\nanomaly_rows = scaled_n1_df[scaled_n1_df['anomalies'] == 1].sample(4000, random_state=42)\nnon_anomaly_rows = scaled_n1_df[scaled_n1_df['anomalies'] == 0].sample(4000, random_state=42)\n\n# Combine the anomaly and non-anomaly rows\nbalanced_df = pd.concat([anomaly_rows, non_anomaly_rows])\n\n# Step 2: Prepare the data\nX = balanced_df.drop(columns=['anomalies'])  # Features (drop the target column)\ny = balanced_df['anomalies']  # Target variable\n\n# Split the data into training and testing sets (optional, but recommended)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Step 3: Train the Random Forest Classifier\nclf = RandomForestClassifier(n_estimators=100, random_state=42)\nclf.fit(X_train, y_train)\n\n# Step 4: Extract the Gini importance for each feature\nimportances = clf.feature_importances_\n\n# Create a DataFrame to display the feature importances\nfeature_importance_df = pd.DataFrame({\n    'Feature': X.columns,\n    'Gini Importance': importances\n}).sort_values(by='Gini Importance', ascending=False)\n\n# Display the feature importances\nprint(feature_importance_df)\n\n# Create a SHAP explainer\n# explainer = shap.Explainer(clf, X)\n\n# # Calculate SHAP values (SHAP interaction values can also be calculated for more insights)\n# shap_values = explainer(X)\n\n# # Plot summary plot to show feature importance\n# shap.summary_plot(shap_values, X)\n\n# # You can also plot feature importance bar plot from SHAP values\n# shap.summary_plot(shap_values, X, plot_type=\"bar\")\n","metadata":{"execution":{"iopub.status.busy":"2024-11-16T03:29:07.155835Z","iopub.execute_input":"2024-11-16T03:29:07.156487Z","iopub.status.idle":"2024-11-16T03:29:09.022364Z","shell.execute_reply.started":"2024-11-16T03:29:07.156454Z","shell.execute_reply":"2024-11-16T03:29:09.021344Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# EDA (anomaly list)","metadata":{}},{"cell_type":"code","source":"# df1=pd.read_csv('/kaggle/input/subject3/10003_26038 (1).tsv',sep='\\t')\n# df1=pd.read_csv('/kaggle/input/subject-5/10015_10063.tsv',sep='\\t')\n# df1=pd.read_csv('/kaggle/input/subject-9/12808_17752.tsv',sep='\\t')\n# df1=pd.read_csv('/kaggle/input/testfinal/10000_17728 (1).tsv',sep='\\t')\ndf1=pd.read_csv('/kaggle/input/9955-18184-tsv/9955_18184.tsv',sep='\\t')\n# df1=pd.read_csv('/kaggle/input/9931-17578-tsv/9931_17578.tsv',sep='\\t')\n# df1=pd.read_csv('/kaggle/input/subject-5/10015_10063.tsv',sep='\\t')\n\n\n\nprint(set(df1['description'].values))","metadata":{"execution":{"iopub.status.busy":"2024-11-16T02:28:50.95684Z","iopub.execute_input":"2024-11-16T02:28:50.957443Z","iopub.status.idle":"2024-11-16T02:28:50.979027Z","shell.execute_reply.started":"2024-11-16T02:28:50.957412Z","shell.execute_reply":"2024-11-16T02:28:50.97811Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# anomalies_list = [\n#     'Central Apnea', 'Grit Teeth or Chew (5 seconds)', 'Breath Hold (10 seconds)',\n#     'Simulate Snore or Hum (5 seconds)','Obstructive Apnea','Obstructive Hypopnea',\n#     'Looking around', 'Eyes Up and Down X5',\n#     'Mixed Apnea', 'Oral Breathing (10 seconds)', 'Oxygen Desaturation',\n#     'Flex Right Foot', 'So', 'Flex Left Foot', 'Eye Blinks X5', 'Bruxism', 'Eyes Left and Right X5'\n# ]\nanomalies_list = ['Oxygen Desaturation']\n# anomalies_list = ['Obstructive Apnea','Mixed Apnea','Central Apnea']\n\n# Assume df1 is your DataFrame containing the annotations\n# Filter annotations dataframe to include only anomalies\nanomalies_df = df1[df1['description'].isin(anomalies_list)]\n\n# Initialize the binary data list with zeros\n# binary_data1 = [0] * len(smoothed_df)\nbinary_data1 = [0] * len(df)\n\n# Iterate over each row in anomalies_df\nfor _, row in anomalies_df.iterrows():\n    start_row = int(row['onset'] * 256)\n    end_row = int((row['onset'] + row['duration']) * 256)\n    # Mark intervals as 1\n    for i in range(start_row, end_row + 1):\n        if i < len(binary_data1):\n            binary_data1[i] = 1\n\n# Add the binary data as a new column to smoothed_df\n# smoothed_df['anomalies'] = binary_data1\ndf['anomalies'] = binary_data1\n\n# Display the updated DataFrame\nprint(df.head())","metadata":{"execution":{"iopub.status.busy":"2024-11-16T02:29:03.467026Z","iopub.execute_input":"2024-11-16T02:29:03.467371Z","iopub.status.idle":"2024-11-16T02:29:07.597152Z","shell.execute_reply.started":"2024-11-16T02:29:03.467344Z","shell.execute_reply":"2024-11-16T02:29:07.596128Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(binary_data1.count(1))","metadata":{"execution":{"iopub.status.busy":"2024-11-16T02:29:14.179901Z","iopub.execute_input":"2024-11-16T02:29:14.180279Z","iopub.status.idle":"2024-11-16T02:29:14.270162Z","shell.execute_reply.started":"2024-11-16T02:29:14.180247Z","shell.execute_reply":"2024-11-16T02:29:14.269196Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Grouping into sleep stages","metadata":{}},{"cell_type":"code","source":"# Define the list of sleep stages\nsleep_stages = ['Sleep stage W', 'Sleep stage N1', 'Sleep stage N2', 'Sleep stage N3','Sleep stage R']\n\n# Initialize empty DataFrames for each sleep stage\nw_df = pd.DataFrame()\nn1_df = pd.DataFrame()\nn2_df = pd.DataFrame()\nn3_df = pd.DataFrame()\nr_df = pd.DataFrame()\n\n# Create a dictionary to map sleep stages to their respective DataFrames\nsleep_stage_dfs = {\n    'Sleep stage W': w_df,\n    'Sleep stage N1': n1_df,\n    'Sleep stage N2': n2_df,\n    'Sleep stage N3': n3_df,\n    'Sleep stage R':r_df\n}\n\n# Filter the annotations dataframe to include only rows for sleep stages\nsleep_stage_df = df1[df1['description'].isin(sleep_stages)]\n\n# Iterate over each row in sleep_stage_df\nfor _, row in sleep_stage_df.iterrows():\n    start_row = int(row['onset'] * 256)  # Calculate the start row in smoothed_df\n    end_row = int((row['onset'] + row['duration']) * 256)  # Calculate the end row in smoothed_df\n    stage = row['description']  # Get the sleep stage (e.g., 'Sleep Stage W')\n    \n    # Get the corresponding rows from smoothed_df and append to the respective DataFrame\n    stage_rows = df.iloc[start_row:end_row + 1]\n    sleep_stage_dfs[stage] = pd.concat([sleep_stage_dfs[stage], stage_rows])\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-11-16T02:31:23.574434Z","iopub.execute_input":"2024-11-16T02:31:23.575206Z","iopub.status.idle":"2024-11-16T02:32:24.030067Z","shell.execute_reply.started":"2024-11-16T02:31:23.575173Z","shell.execute_reply":"2024-11-16T02:32:24.029149Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Now w_df, n1_df, n2_df, and n3_df will contain the corresponding rows from smoothed_df\nw_df = sleep_stage_dfs['Sleep stage W']\nn1_df = sleep_stage_dfs['Sleep stage N1']\nn2_df = sleep_stage_dfs['Sleep stage N2']\nn3_df = sleep_stage_dfs['Sleep stage N3']\nr_df =  sleep_stage_dfs['Sleep stage R']\n\n# Optionally, display the DataFrames for each sleep stage\nprint(\"W Stage DataFrame:\")\nprint(w_df)\nprint(\"\\nN1 Stage DataFrame:\")\nprint(n1_df)\nprint(\"\\nN2 Stage DataFrame:\")\nprint(n2_df)\nprint(\"\\nN3 Stage DataFrame:\")\nprint(n3_df)\nprint(\"\\nR Stage DataFrame:\")\nprint(r_df)","metadata":{"execution":{"iopub.status.busy":"2024-11-16T02:32:55.444498Z","iopub.execute_input":"2024-11-16T02:32:55.445163Z","iopub.status.idle":"2024-11-16T02:32:56.060509Z","shell.execute_reply.started":"2024-11-16T02:32:55.445131Z","shell.execute_reply":"2024-11-16T02:32:56.059512Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"w_df.columns,len(w_df)","metadata":{"execution":{"iopub.status.busy":"2024-11-16T02:33:41.849176Z","iopub.execute_input":"2024-11-16T02:33:41.849685Z","iopub.status.idle":"2024-11-16T02:33:41.864747Z","shell.execute_reply.started":"2024-11-16T02:33:41.849629Z","shell.execute_reply":"2024-11-16T02:33:41.8617Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# minimum SpO2","metadata":{}},{"cell_type":"code","source":"print(\"Minimum SpO2 value for sleep stage W as an anomaly:\")\nmin_SpO2_w = w_df[w_df['anomalies'] == 1]['SpO2'].min()\nprint(min_SpO2_w,w_df['SpO2'].min())\n\nprint(\"Minimum SpO2 value for sleep stage N1 as an anomaly:\")\nmin_SpO2_n1 = n1_df[n1_df['anomalies'] == 1]['SpO2'].min()\nprint(min_SpO2_n1,n1_df['SpO2'].min())\n\nprint(\"Minimum SpO2 value for sleep stage N2 as an anomaly:\")\nmin_SpO2_n2 = n2_df[n2_df['anomalies'] == 1]['SpO2'].min()\nprint(min_SpO2_n2,n2_df['SpO2'].min())\n\nprint(\"Minimum SpO2 value for sleep stage N3 as an anomaly:\")\nmin_SpO2_n3 = n3_df[n3_df['anomalies'] == 1]['SpO2'].min()\nprint(min_SpO2_n3,n3_df['SpO2'].min())\n\nprint(\"Minimum SpO2 value for sleep stage R as an anomaly:\")\nmin_SpO2_r = r_df[r_df['anomalies'] == 1]['SpO2'].min()\nprint(min_SpO2_r,r_df['SpO2'].min())","metadata":{"execution":{"iopub.status.busy":"2024-11-05T06:00:08.479657Z","iopub.execute_input":"2024-11-05T06:00:08.480499Z","iopub.status.idle":"2024-11-05T06:00:08.554435Z","shell.execute_reply.started":"2024-11-05T06:00:08.480469Z","shell.execute_reply":"2024-11-05T06:00:08.553537Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"w_df = w_df[w_df['SpO2'] >= max(70,min_SpO2_w)].reset_index(drop=True)\nn1_df = n1_df[n1_df['SpO2'] >= min_SpO2_n1].reset_index(drop=True)\nn2_df = n2_df[n2_df['SpO2'] >= min_SpO2_n2].reset_index(drop=True)\nn3_df = n3_df[n3_df['SpO2'] >= min_SpO2_n3].reset_index(drop=True)\nr_df = r_df[r_df['SpO2'] >= min_SpO2_r].reset_index(drop=True)\n\n# w_df = w_df[w_df['SpO2'] >= 78.58853130960594].reset_index(drop=True)\n# n1_df = n1_df[n1_df['SpO2'] >= 0.0040582768556589].reset_index(drop=True)\n# n2_df = n2_df[n2_df['SpO2'] >= 86.44941357899437].reset_index(drop=True)\n# n3_df = n3_df[n3_df['SpO2'] >= 92.34203157339394].reset_index(drop=True)\n# r_df = r_df[r_df['SpO2'] >= 92.34203157339394].reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2024-11-05T06:00:09.916408Z","iopub.execute_input":"2024-11-05T06:00:09.917252Z","iopub.status.idle":"2024-11-05T06:00:10.790629Z","shell.execute_reply.started":"2024-11-05T06:00:09.917222Z","shell.execute_reply":"2024-11-05T06:00:10.789845Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = df[df['SpO2'] >= 70].reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2024-11-05T06:00:14.559103Z","iopub.execute_input":"2024-11-05T06:00:14.559518Z","iopub.status.idle":"2024-11-05T06:00:15.993235Z","shell.execute_reply.started":"2024-11-05T06:00:14.559486Z","shell.execute_reply":"2024-11-05T06:00:15.992407Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# len(df)\ntotal = len(w_df)+len(n1_df)+len(n2_df)+len(n3_df)+len(r_df)\nprint(total,len(df))                                        ","metadata":{"execution":{"iopub.status.busy":"2024-11-16T02:36:17.990933Z","iopub.execute_input":"2024-11-16T02:36:17.991971Z","iopub.status.idle":"2024-11-16T02:36:17.997697Z","shell.execute_reply.started":"2024-11-16T02:36:17.991935Z","shell.execute_reply":"2024-11-16T02:36:17.996479Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Percentage of W stage:\",((len(w_df)/(len(w_df)+len(n1_df)+len(n2_df)+len(n3_df)+len(r_df)))*100))\nprint(\"Percentage of N1 stage:\",((len(n1_df)/(len(w_df)+len(n1_df)+len(n2_df)+len(n3_df)+len(r_df)))*100))\nprint(\"Percentage of N2 stage:\",((len(n2_df)/(len(w_df)+len(n1_df)+len(n2_df)+len(n3_df)+len(r_df)))*100))\nprint(\"Percentage of N3 stage:\",((len(n3_df)/(len(w_df)+len(n1_df)+len(n2_df)+len(n3_df)+len(r_df)))*100))\nprint(\"Percentage of R stage:\",((len(r_df)/(len(w_df)+len(n1_df)+len(n2_df)+len(n3_df)+len(r_df)))*100))","metadata":{"execution":{"iopub.status.busy":"2024-11-16T02:34:58.331542Z","iopub.execute_input":"2024-11-16T02:34:58.332552Z","iopub.status.idle":"2024-11-16T02:34:58.340844Z","shell.execute_reply.started":"2024-11-16T02:34:58.332516Z","shell.execute_reply":"2024-11-16T02:34:58.339723Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(n3_df),n3_df.columns","metadata":{"execution":{"iopub.status.busy":"2024-10-27T10:56:29.056968Z","iopub.execute_input":"2024-10-27T10:56:29.057401Z","iopub.status.idle":"2024-10-27T10:56:29.066752Z","shell.execute_reply.started":"2024-10-27T10:56:29.057369Z","shell.execute_reply":"2024-10-27T10:56:29.065198Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# w_df = w_df.drop(\"EtCO2\", axis='columns')\n# n1_df = n1_df.drop(\"EtCO2\", axis='columns')\n# n2_df = n2_df.drop(\"EtCO2\", axis='columns')\n# n3_df = n3_df.drop(\"EtCO2\", axis='columns')\n# r_df = r_df.drop(\"EtCO2\", axis='columns')","metadata":{"execution":{"iopub.status.busy":"2024-10-20T13:10:26.293753Z","iopub.execute_input":"2024-10-20T13:10:26.294613Z","iopub.status.idle":"2024-10-20T13:10:26.29944Z","shell.execute_reply.started":"2024-10-20T13:10:26.294559Z","shell.execute_reply":"2024-10-20T13:10:26.29819Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.shape\ndf.columns","metadata":{"execution":{"iopub.status.busy":"2024-10-27T17:30:58.773509Z","iopub.execute_input":"2024-10-27T17:30:58.773909Z","iopub.status.idle":"2024-10-27T17:30:58.780237Z","shell.execute_reply.started":"2024-10-27T17:30:58.773877Z","shell.execute_reply":"2024-10-27T17:30:58.779297Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n\n# Assuming 'anomalies' is your ground truth column in wavelet_features_df\n# and 'anomaly' is the predicted label from the K-means clustering step.\n\n# Step 1: Map the 'anomalous' and 'non-anomalous' values to binary labels for comparison\n# Ground truth: 'anomalous' should map to 1, 'non-anomalous' to 0\nwavelet_features_df['ground_truth'] = wavelet_features_df['anomalies'].map(lambda x: 1 if x == 'anomalous' else 0)\nwavelet_features_df['predicted'] = wavelet_features_df['anomaly'].map(lambda x: 1 if x == 'anomalous' else 0)\n\n# Step 2: Calculate evaluation metrics\naccuracy = accuracy_score(wavelet_features_df['ground_truth'], wavelet_features_df['predicted'])\nprecision = precision_score(wavelet_features_df['ground_truth'], wavelet_features_df['predicted'])\nrecall = recall_score(wavelet_features_df['ground_truth'], wavelet_features_df['predicted'])\nf1 = f1_score(wavelet_features_df['ground_truth'], wavelet_features_df['predicted'])\nconf_matrix = confusion_matrix(wavelet_features_df['ground_truth'], wavelet_features_df['predicted'])\nbalanced_accuracy = balanced_accuracy_score(wavelet_features_df['ground_truth'], wavelet_features_df['predicted'])\n\n# Print the evaluation results\nprint(\"Accuracy:\", accuracy)\nprint(\"Precision:\", precision)\nprint(\"Recall:\", recall)\nprint(\"F1 Score:\", f1)\ntn, fp, fn, tp = confusion_matrix(wavelet_features_df['ground_truth'], wavelet_features_df['predicted']).ravel()\nspecificity = tn / (tn + fp)\nprint(\"Specificity:\", specificity)\nprint(\"Balanced Accuracy: \",(specificity+recall)/2)\nprint(\"Confusion Matrix:\\n\", conf_matrix)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T15:50:50.769476Z","iopub.execute_input":"2024-11-14T15:50:50.770288Z","iopub.status.idle":"2024-11-14T15:50:50.811788Z","shell.execute_reply.started":"2024-11-14T15:50:50.770256Z","shell.execute_reply":"2024-11-14T15:50:50.8109Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import balanced_accuracy_score, classification_report\nfrom sklearn.utils import resample\nfrom sklearn.metrics import accuracy_score, balanced_accuracy_score, confusion_matrix\n\naccuracies = []\nfor percent in range(30,90,10):\n    percent /= 100\n    percent_index = int(percent * len(wavelet_features_df))\n    \n    # Splitting the data \n    train_df = wavelet_features_df.iloc[:percent_index]\n    test_df = wavelet_features_df.iloc[percent_index:]\n    \n    # Separate the minority and majority classes in the training set\n    anomalies = train_df[train_df['anomalies'] == 1]\n    non_anomalies = train_df[train_df['anomalies'] == 0]\n    \n    # Downsampling the majority class to match the minority class size\n    non_anomalies_downsampled = resample(non_anomalies, \n                                         replace=False,  # sample without replacement\n                                         n_samples=len(anomalies),  # match number of anomalies\n                                         random_state=42)  # fixed seed for reproducibility\n    \n    # Create a balanced training dataset\n    balanced_train_df = pd.concat([anomalies, non_anomalies_downsampled])\n    \n    # Separate features and target for training and testing\n    # X_train = balanced_train_df.drop(columns=['anomalies'])\n    # y_train = balanced_train_df['anomalies']\n    # X_test = test_df.drop(columns=['anomalies'])\n    # y_test = test_df['anomalies']\n    \n    # Split data into features (X) and target (y)\n    X = wavelet_features_df.drop(columns=['anomalies'])\n    y = wavelet_features_df['anomalies']\n    \n    # # Split data into training and testing sets\n    # X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1-percent, random_state=42)\n    \n    # Train an SVM model\n    svm_model = SVC(kernel='rbf', random_state=42)\n    svm_model.fit(X_train, y_train)\n    \n    # Predict on the test set\n    y_pred = svm_model.predict(X_test)\n    \n    # Calculate accuracy\n    accuracy = accuracy_score(y_test, y_pred)\n    balanced_accuracy = balanced_accuracy_score(y_test, y_pred)\n    \n    # Calculate specificity\n    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n    specificity = tn / (tn + fp)\n    \n    # Print results\n    # print(\"Accuracy:\", accuracy)\n    print(\"Balanced Accuracy:\", balanced_accuracy)\n    # print(\"Specificity:\", specificity)\n    accuracies.append(balanced_accuracy)\n    \nprint(max(accuracies))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T04:38:43.048505Z","iopub.execute_input":"2024-11-15T04:38:43.048993Z","iopub.status.idle":"2024-11-15T04:38:58.32337Z","shell.execute_reply.started":"2024-11-15T04:38:43.048951Z","shell.execute_reply":"2024-11-15T04:38:58.322326Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom scipy.fft import fft\n\n# Assuming smoothed_n2_df is your DataFrame with all the relevant columns\n# List of columns to process\ncolumns_to_process = ['EOG LOC-M2', 'EOG ROC-M1', 'EMG Chin2-Chin1', 'EEG F3-M2', 'EEG F4-M1', 'EEG C3-M2', \n                      'EEG C4-M1', 'EEG O1-M2', 'EEG O2-M1', 'EEG CZ-O1', 'EMG LLeg-RLeg', 'ECG EKG2-EKG', \n                      'Snore', 'Resp PTAF', 'Resp Airflow', 'Resp Thoracic', 'Resp Abdominal', 'SpO2', 'Rate',  'Capno', 'Resp Rate', 'C-flow', 'Tidal Vol', 'Pressure']\n\n# Parameters\nsegment_size = 1280  # 10 seconds segment (1280 rows)\nanomaly_threshold = 0.10  # 10% anomaly threshold\n\n# Lists to store results\nfeature_rows = []\n\n# Loop through the data and segment it for each column\nfor start_idx in range(0, len(df), segment_size):\n    end_idx = start_idx + segment_size\n    \n    # Ensure segment does not go out of bounds\n    if end_idx > len(df):\n        break\n    \n    # Store features for each column in this segment\n    segment_features = {}\n    \n    # Loop through each column\n    for col in columns_to_process:\n        # Extract the data and annotations for the current column\n        segment = df[col].iloc[start_idx:end_idx].values\n        segment_annotations = df['anomalies'].iloc[start_idx:end_idx].values\n\n        # Check if the segment contains more than 10% anomalies\n        if np.mean(segment_annotations) > anomaly_threshold:\n            segment_label = 1  # Anomalous\n        else:\n            segment_label = 0  # Non-anomalous\n\n        # Apply FFT to the segment\n        fft_segment = fft(segment)\n        \n        # Compute the frequency bins\n        n = len(segment)\n        freqs = np.fft.fftfreq(n)\n\n        # Calculate the amplitude spectrum\n        amplitude_segment = np.abs(fft_segment)\n        # print(amplitude_segment.shape)\n\n        # Extract the dominant frequency (excluding zero frequency)\n        dominant_freq = freqs[np.argmax(amplitude_segment[1:n//2])]\n\n        # Store features: dominant frequency, mean amplitude, and mean value\n        segment_features[f'{col}_dominant_freq'] = dominant_freq\n        segment_features[f'{col}_mean_amplitude'] = np.mean(amplitude_segment)  # Mean of the amplitude\n        segment_features[f'{col}_mean_value'] = np.mean(segment)  # Mean of the original segment\n\n    # Add the label\n    segment_features['label'] = segment_label\n    \n    # Append to feature rows\n    feature_rows.append(segment_features)\n\n# Create a DataFrame from the extracted features\nfeature_df = pd.DataFrame(feature_rows)\n\n# Display the resulting DataFrame\n# print(feature_df)","metadata":{"execution":{"iopub.status.busy":"2024-10-20T13:48:02.964656Z","iopub.execute_input":"2024-10-20T13:48:02.965308Z","iopub.status.idle":"2024-10-20T13:48:33.488324Z","shell.execute_reply.started":"2024-10-20T13:48:02.965277Z","shell.execute_reply":"2024-10-20T13:48:33.487527Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# print(feature_df[feature_df['label']==1],\"   len  =.  \",len(feature_df[feature_df['label']==1]))\nfeature_df.columns","metadata":{"execution":{"iopub.status.busy":"2024-10-19T14:29:52.584243Z","iopub.execute_input":"2024-10-19T14:29:52.585037Z","iopub.status.idle":"2024-10-19T14:29:52.591969Z","shell.execute_reply.started":"2024-10-19T14:29:52.584997Z","shell.execute_reply":"2024-10-19T14:29:52.591081Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, balanced_accuracy_score\n# Separate features and labels\n# X = feature_df[['dominant_freq', 'mean_amplitude', 'mean_spo2']]\nX = feature_df[['EOG LOC-M2_dominant_freq', 'EOG LOC-M2_mean_amplitude',\n       'EOG LOC-M2_mean_value', 'EOG ROC-M1_dominant_freq',\n       'EOG ROC-M1_mean_amplitude', 'EOG ROC-M1_mean_value',\n       'EMG Chin2-Chin1_dominant_freq', 'EMG Chin2-Chin1_mean_amplitude',\n       'EMG Chin2-Chin1_mean_value', 'EEG F3-M2_dominant_freq',\n       'EEG F3-M2_mean_amplitude', 'EEG F3-M2_mean_value',\n       'EEG F4-M1_dominant_freq', 'EEG F4-M1_mean_amplitude',\n       'EEG F4-M1_mean_value', 'EEG C3-M2_dominant_freq',\n       'EEG C3-M2_mean_amplitude', 'EEG C3-M2_mean_value',\n       'EEG C4-M1_dominant_freq', 'EEG C4-M1_mean_amplitude',\n       'EEG C4-M1_mean_value', 'EEG O1-M2_dominant_freq',\n       'EEG O1-M2_mean_amplitude', 'EEG O1-M2_mean_value',\n       'EEG O2-M1_dominant_freq', 'EEG O2-M1_mean_amplitude',\n       'EEG O2-M1_mean_value', 'EEG CZ-O1_dominant_freq',\n       'EEG CZ-O1_mean_amplitude', 'EEG CZ-O1_mean_value',\n       'EMG LLeg-RLeg_dominant_freq', 'EMG LLeg-RLeg_mean_amplitude',\n       'EMG LLeg-RLeg_mean_value', 'ECG EKG2-EKG_dominant_freq',\n       'ECG EKG2-EKG_mean_amplitude', 'ECG EKG2-EKG_mean_value',\n       'Snore_dominant_freq', 'Snore_mean_amplitude', 'Snore_mean_value',\n       'Resp PTAF_dominant_freq', 'Resp PTAF_mean_amplitude',\n       'Resp PTAF_mean_value', 'Resp Airflow_dominant_freq',\n       'Resp Airflow_mean_amplitude', 'Resp Airflow_mean_value',\n       'Resp Thoracic_dominant_freq', 'Resp Thoracic_mean_amplitude',\n       'Resp Thoracic_mean_value', 'Resp Abdominal_dominant_freq',\n       'Resp Abdominal_mean_amplitude', 'Resp Abdominal_mean_value',\n       'SpO2_dominant_freq', 'SpO2_mean_amplitude', 'SpO2_mean_value',\n       'Rate_dominant_freq', 'Rate_mean_amplitude', 'Rate_mean_value',\n       'Capno_dominant_freq', 'Capno_mean_amplitude', 'Capno_mean_value',\n       'Resp Rate_dominant_freq', 'Resp Rate_mean_amplitude',\n       'Resp Rate_mean_value', 'C-flow_dominant_freq', 'C-flow_mean_amplitude',\n       'C-flow_mean_value', 'Tidal Vol_dominant_freq',\n       'Tidal Vol_mean_amplitude', 'Tidal Vol_mean_value',\n       'Pressure_dominant_freq', 'Pressure_mean_amplitude',\n       'Pressure_mean_value']]\ny = feature_df['label']\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42,stratify=y)\n\n# Initialize the Random Forest Classifier\nrfc = RandomForestClassifier(random_state=42)\n\n# Train the model\nrfc.fit(X_train, y_train)\n\n# Predict on the test set\ny_pred = rfc.predict(X_test)\n\n# Evaluate the model\nprint(classification_report(y_test, y_pred))\nbalanced_acc = balanced_accuracy_score(y_test, y_pred)\nprint(f\"Balanced Accuracy: {balanced_acc:.4f}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-19T14:31:04.248875Z","iopub.execute_input":"2024-10-19T14:31:04.249238Z","iopub.status.idle":"2024-10-19T14:31:06.828112Z","shell.execute_reply.started":"2024-10-19T14:31:04.249209Z","shell.execute_reply":"2024-10-19T14:31:06.827207Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2024-10-18T13:21:25.526953Z","iopub.execute_input":"2024-10-18T13:21:25.527343Z","iopub.status.idle":"2024-10-18T13:21:25.553868Z","shell.execute_reply.started":"2024-10-18T13:21:25.527306Z","shell.execute_reply":"2024-10-18T13:21:25.55295Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom scipy.fft import fft\n\n# Assuming your dataset is in a pandas DataFrame called 'df' and it contains the signal columns.\n\n# Define the sampling frequency and segment size\nsampling_frequency = 256  # Hz\nsegment_size = 1280  # 5 seconds of data (1280 rows)\n\ndef compute_fft(segment):\n    # Ensure the segment is a numpy array\n    segment = np.array(segment)\n    \n    # Check if there are any missing values\n    if np.isnan(segment).any():\n        segment = np.nan_to_num(segment)  # Replace NaNs with 0\n    \n    # Apply FFT\n    fft_result = fft(segment)\n    \n    # Calculate power spectrum\n    power_spectrum = np.abs(fft_result)**2\n    \n    return power_spectrum[:len(power_spectrum)//2]  # Only take positive frequencies\n\n# Select only numeric columns for signal data\nsignal_columns = ['SpO2']  # Modify this based on your actual signal channels\n\n# Initialize an empty list to store the amplitude data (power spectrum)\namplitude_list = []\n\n# Loop through each channel and segment, and apply FFT\nfor channel in signal_columns:\n    channel_amplitudes = []  # List to hold amplitude data for this channel\n    for start in range(0, len(df), segment_size):\n        segment = df[channel].iloc[start:start + segment_size]\n        if len(segment) == segment_size:  # Ensure the segment has the correct size\n            power_spectrum = compute_fft(segment)\n            channel_amplitudes.append(power_spectrum)\n    \n    amplitude_list.append(channel_amplitudes)\n\n# Convert the list to a numpy array\n# Shape will be (num_channels, num_segments, segment_size/2) since we take only positive frequencies\namplitude_array = np.array(amplitude_list)\n\n# Transpose the array to have shape (num_segments, segment_size/2, num_channels)\namplitude_array = np.transpose(amplitude_array, (1, 2, 0))\n\n# Check the shape of the amplitude array\nprint(\"Amplitude array shape:\", amplitude_array.shape)\n\n# Now you can use amplitude_array to train the autoencoder\n# autoencoder.fit(amplitude_array, amplitude_array, epochs=50, batch_size=32, validation_split=0.2)\n\n\n\n# import numpy as np\n# import pandas as pd\n# from scipy.fft import fft\n\n# # Assuming your dataset is in a pandas DataFrame called 'df' and it contains the signal columns.\n\n# # Define the sampling frequency and segment size\n# sampling_frequency = 256  # Hz\n# segment_size = 1280  # 5 seconds of data (1280 rows)\n\n# def compute_fft(segment):\n#     # Ensure the segment is a numpy array\n#     segment = np.array(segment)\n    \n#     # Debugging information to see the input segment\n#     print(\"Input segment length: \", len(segment))\n#     # print(\"First 5 values of segment: \", segment[:5])\n    \n#     # Check if there are any missing values\n#     if np.isnan(segment).any():\n#         print(\"Warning: Segment contains NaN values. Filling NaNs with 0.\")\n#         segment = np.nan_to_num(segment)  # Replace NaNs with 0\n    \n#     # Apply FFT\n#     print(\"Applying FFT...\")\n#     fft_result = fft(segment)\n#     print(\"FFT result shape: \", fft_result.shape)\n    \n#     # Calculate power spectrum\n#     power_spectrum = np.abs(fft_result)**2\n    \n#     # Calculate frequencies\n#     frequencies = np.fft.fftfreq(len(segment), d=1/sampling_frequency)\n    \n#     # Debugging information to see the output of FFT and frequencies\n#     print(\"First 5 FFT values: \", fft_result[:5])\n#     print(\"First 5 frequencies: \", frequencies[:5])\n    \n#     return frequencies, power_spectrum\n\n\n# # Select only numeric columns for signal data\n# signal_columns = df.select_dtypes(include=[np.number]).columns\n# print(\"signal_columns = \",signal_columns)\n# signal_columns = ['SpO2']\n\n# # Apply FFT to each channel in the dataset in segments of 1280 rows\n# fft_features = []\n\n# # Loop through each channel (assuming channels are in columns)\n# for channel in signal_columns:\n#     for start in range(0, len(df), segment_size):\n#         segment = df[channel].iloc[start:start + segment_size]\n#         if len(segment) == segment_size:  # Ensure the segment has the correct size\n#             # print(\"segment = \",segment)\n#             print(\"---------\\n\")\n#             freqs, power = compute_fft(segment)\n#             fft_features.append({\n#                 'channel': channel,\n#                 'frequencies': freqs[:len(freqs)//2],  # Only take positive frequencies\n#                 'power_spectrum': power[:len(power)//2]  # Positive frequency components\n#             })\n\n# # Convert FFT features into a suitable format for further analysis\n# fft_df = pd.DataFrame(fft_features)\n\n# # Display the FFT DataFrame\n# print(fft_df)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-20T13:27:57.563744Z","iopub.execute_input":"2024-10-20T13:27:57.564459Z","iopub.status.idle":"2024-10-20T13:27:58.463566Z","shell.execute_reply.started":"2024-10-20T13:27:57.564427Z","shell.execute_reply":"2024-10-20T13:27:58.462405Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# df.columns\namplitude_array[:1].shape","metadata":{"execution":{"iopub.status.busy":"2024-10-20T13:29:12.62602Z","iopub.execute_input":"2024-10-20T13:29:12.627018Z","iopub.status.idle":"2024-10-20T13:29:12.634767Z","shell.execute_reply.started":"2024-10-20T13:29:12.626978Z","shell.execute_reply":"2024-10-20T13:29:12.633726Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Pre-processing","metadata":{}},{"cell_type":"markdown","source":"## Smoothing each sleep stage df","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport scipy.signal as signal\nimport matplotlib.pyplot as plt\n\n# Example settings\nfs = 256  # Sampling frequency in Hz\nepoch_duration = 30  # Each EEG epoch is 30 seconds long\nnyquist = 0.5 * fs  # Nyquist frequency\n\n# Frequencies to eliminate (60 Hz and 120 Hz)\nfreqs_to_eliminate = [60, 120]\n\n# Design a 3rd-order Butterworth bandstop filter\ndef butter_bandstop(lowcut, highcut, fs, order=3):\n    low = lowcut / nyquist\n    high = highcut / nyquist\n    b, a = signal.butter(order, [low, high], btype='bandstop')\n    return b, a\n\n# Apply the filter to the EEG signal\ndef apply_bandstop_filter(eeg_signal, fs, freqs_to_eliminate, order=3):\n    filtered_signal = eeg_signal.copy()  # Copy to avoid modifying the original\n    for freq in freqs_to_eliminate:\n        # Create bandstop filters around the target frequencies\n        lowcut = freq - 1  # 1 Hz below the target frequency\n        highcut = freq + 1  # 1 Hz above the target frequency\n        b, a = butter_bandstop(lowcut, highcut, fs, order)\n        # Apply filter to the signal\n        filtered_signal = signal.filtfilt(b, a, filtered_signal)\n    return filtered_signal\n\n# Assuming df is your DataFrame with the 7 EEG columns\neeg_columns = ['EEG F3-M2', 'EEG F4-M1', 'EEG C3-M2', 'EEG C4-M1', 'EEG O1-M2', 'EEG O2-M1', 'EEG CZ-O1']\n\n# Apply the bandstop filter to each EEG column and store the filtered signals\nfiltered_signals = {}\n\n# Apply the bandstop filter and update the EEG columns in the DataFrame\ndef update_eeg_columns_with_filtered_signals(df, fs, freqs_to_eliminate, eeg_columns, order=3):\n#     filtered_signals = {}\n    for column in eeg_columns:\n        eeg_signal = df[column].values  # Extract EEG signal from the DataFrame\n        filtered_signal = apply_bandstop_filter(eeg_signal, fs, freqs_to_eliminate, order)\n        df[column] = filtered_signal  # Update the DataFrame with the filtered signal\n    return df\n\nw_df = update_eeg_columns_with_filtered_signals(w_df, fs, freqs_to_eliminate, eeg_columns)\nn1_df = update_eeg_columns_with_filtered_signals(n1_df, fs, freqs_to_eliminate, eeg_columns)\nn2_df = update_eeg_columns_with_filtered_signals(n2_df, fs, freqs_to_eliminate, eeg_columns)\nn3_df = update_eeg_columns_with_filtered_signals(n3_df, fs, freqs_to_eliminate, eeg_columns)\nr_df = update_eeg_columns_with_filtered_signals(r_df, fs, freqs_to_eliminate, eeg_columns)\n\n# Plot original vs filtered signal for each column\n# plt.figure(figsize=(14, 12))\n# for i, column in enumerate(eeg_columns):\n#     plt.subplot(len(eeg_columns), 2, 2*i + 1)\n#     plt.plot(df.index, df[column])\n#     plt.title(f'Original EEG Signal - {column}')\n    \n#     plt.subplot(len(eeg_columns), 2, 2*i + 2)\n#     plt.plot(df.index, filtered_signals[column])\n#     plt.title(f'Filtered EEG Signal - {column}')\n    \n# plt.tight_layout()\n# plt.show()\n\n","metadata":{"execution":{"iopub.status.busy":"2024-11-16T02:38:32.237014Z","iopub.execute_input":"2024-11-16T02:38:32.237421Z","iopub.status.idle":"2024-11-16T02:38:37.149074Z","shell.execute_reply.started":"2024-11-16T02:38:32.237389Z","shell.execute_reply":"2024-11-16T02:38:37.147471Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from scipy.signal import savgol_filter\nprint(df.columns)\n# Define the common window size\n# List of columns to smooth (excluding 'anomalies')\ncolumns_to_smooth = n3_df.columns.difference(['anomalies'])\n\n# Apply the moving average filter to each column except 'anomalies'\nsmoothed_w_df = w_df.copy()  # Create a copy to avoid modifying the original DataFrame\nsmoothed_n1_df = n1_df.copy()  # Create a copy to avoid modifying the original DataFrame\nsmoothed_n2_df = n2_df.copy()  # Create a copy to avoid modifying the original DataFrame\nsmoothed_n3_df = n3_df.copy()  # Create a copy to avoid modifying the original DataFrame\nsmoothed_r_df = r_df.copy()  # Create a copy to avoid modifying the original DataFrame\n# Create a copy to avoid modifying the original DataFrame\ncommon_window_size = 512  # Approximately 2 seconds of data\n\nfor col in columns_to_smooth:\n    smoothed_w_df[col] = w_df[col].rolling(window=common_window_size, min_periods=1).mean()\n    smoothed_n1_df[col] = n1_df[col].rolling(window=common_window_size, min_periods=1).mean()\n    smoothed_n2_df[col] = n2_df[col].rolling(window=common_window_size, min_periods=1).mean()\n    smoothed_n3_df[col] = n3_df[col].rolling(window=common_window_size, min_periods=1).mean()\n    smoothed_r_df[col] = r_df[col].rolling(window=common_window_size, min_periods=1).mean()\n    \nprint(smoothed_w_df.head())  # Check the result\nprint(smoothed_n1_df.head())  # Check the result\nprint(smoothed_n2_df.head())  # Check the result\nprint(smoothed_n3_df.head())  # Check the result\nprint(smoothed_r_df.head())    # Create a copy to avoid modifying the original DataFrame","metadata":{"execution":{"iopub.status.busy":"2024-11-16T02:38:46.469441Z","iopub.execute_input":"2024-11-16T02:38:46.470506Z","iopub.status.idle":"2024-11-16T02:38:51.70245Z","shell.execute_reply.started":"2024-11-16T02:38:46.470473Z","shell.execute_reply":"2024-11-16T02:38:51.701342Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"smoothed_w_df.columns,len(smoothed_w_df)","metadata":{"execution":{"iopub.status.busy":"2024-11-16T02:38:51.704506Z","iopub.execute_input":"2024-11-16T02:38:51.704917Z","iopub.status.idle":"2024-11-16T02:38:51.712082Z","shell.execute_reply.started":"2024-11-16T02:38:51.70488Z","shell.execute_reply":"2024-11-16T02:38:51.711044Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(w_df),len(n1_df),len(n2_df),len(n3_df),len(r_df)","metadata":{"execution":{"iopub.status.busy":"2024-11-16T02:38:57.218738Z","iopub.execute_input":"2024-11-16T02:38:57.219088Z","iopub.status.idle":"2024-11-16T02:38:57.225896Z","shell.execute_reply.started":"2024-11-16T02:38:57.219059Z","shell.execute_reply":"2024-11-16T02:38:57.224849Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(smoothed_w_df),len(smoothed_n1_df),len(smoothed_n2_df),len(smoothed_n3_df),len(smoothed_r_df)","metadata":{"execution":{"iopub.status.busy":"2024-11-16T02:39:00.647636Z","iopub.execute_input":"2024-11-16T02:39:00.648514Z","iopub.status.idle":"2024-11-16T02:39:00.654285Z","shell.execute_reply.started":"2024-11-16T02:39:00.648483Z","shell.execute_reply":"2024-11-16T02:39:00.653485Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Standardizing/scaling the per sleep stage df","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import RobustScaler\nimport pandas as pd\n\n# Initialize the RobustScaler\nscaler = RobustScaler()\n\n# List of DataFrames to scale\n# dfs_to_scale = [smoothed_w_df, smoothed_n1_df,smoothed_n3_df, smoothed_r_df]\ndfs_to_scale = [smoothed_w_df, smoothed_n1_df, smoothed_n2_df, smoothed_n3_df, smoothed_r_df]\n\n# Perform Robust Scaling on each DataFrame\nscaled_dfs = []\nfor df in dfs_to_scale:\n    # Apply scaling (excluding 'anomalies' column, if it's in the DataFrame)\n    if 'anomalies' in df.columns:\n        features = df.drop(columns=['anomalies'])\n        scaled_features = scaler.fit_transform(features)\n        scaled_df = pd.DataFrame(scaled_features, columns=features.columns)\n        scaled_df['anomalies'] = df['anomalies'].values  # Reattach 'anomalies' column\n    else:\n        scaled_features = scaler.fit_transform(df)\n        scaled_df = pd.DataFrame(scaled_features, columns=df.columns)\n\n    scaled_dfs.append(scaled_df)\n\n# Assign scaled DataFrames to variables\n# scaled_w_df, scaled_n1_df, scaled_n3_df, scaled_r_df = scaled_dfs\nscaled_w_df, scaled_n1_df, scaled_n2_df, scaled_n3_df, scaled_r_df = scaled_dfs\n\n# Print the heads of the scaled DataFrames\nprint(scaled_w_df.head())\nprint(scaled_n1_df.head())\nprint(scaled_n2_df.head())\nprint(scaled_n3_df.head())\nprint(scaled_r_df.head())\n\n","metadata":{"execution":{"iopub.status.busy":"2024-11-16T03:27:27.74879Z","iopub.execute_input":"2024-11-16T03:27:27.749204Z","iopub.status.idle":"2024-11-16T03:27:32.631847Z","shell.execute_reply.started":"2024-11-16T03:27:27.749172Z","shell.execute_reply":"2024-11-16T03:27:32.630813Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"scaled_r_df.columns,len(scaled_r_df.columns),len(scaled_n2_df)","metadata":{"execution":{"iopub.status.busy":"2024-11-16T03:27:32.633435Z","iopub.execute_input":"2024-11-16T03:27:32.633764Z","iopub.status.idle":"2024-11-16T03:27:32.640684Z","shell.execute_reply.started":"2024-11-16T03:27:32.633736Z","shell.execute_reply":"2024-11-16T03:27:32.639647Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# scaled_w_df=scaled_w_df.drop(columns=['Patient Event','anomalies'])\n# scaled_n1_df=scaled_n1_df.drop(columns=['Patient Event','anomalies'])\n# scaled_n2_df=scaled_n2_df.drop(columns=['Patient Event','anomalies'])\n# scaled_n3_df=scaled_n3_df.drop(columns=['Patient Event','anomalies'])\n# scaled_r_df=scaled_r_df.drop(columns=['Patient Event','anomalies'])\n\n# # scaled_w_df.drop('Patient Event')\n# scaled_n3_df=scaled_n3_df.drop(columns=['anomalies'])\n# scaled_r_df=scaled_r_df.drop(columns=['anomalies'])\n# scaled_w_df=scaled_w_df.drop(columns=['anomalies'])\n# scaled_n1_df=scaled_n1_df.drop(columns=['anomalies'])\n# scaled_n2_df=scaled_n2_df.drop(columns=['anomalies'])\n\n# scaled_w_df=scaled_w_df.drop(columns=['EtCO2','anomalies'])\n# scaled_n1_df=scaled_n1_df.drop(columns=['EtCO2','anomalies'])\n# scaled_n2_df=scaled_n2_df.drop(columns=['EtCO2','anomalies'])\n# scaled_n3_df=scaled_n3_df.drop(columns=['EtCO2','anomalies'])\n# scaled_r_df=scaled_r_df.drop(columns=['EtCO2','anomalies'])\n\nscaled_w_df=scaled_w_df.drop(columns=['Patient Event','EtCO2','anomalies'])\nscaled_n1_df=scaled_n1_df.drop(columns=['Patient Event','EtCO2','anomalies'])\nscaled_n2_df=scaled_n2_df.drop(columns=['Patient Event','EtCO2','anomalies'])\nscaled_n3_df=scaled_n3_df.drop(columns=['Patient Event','EtCO2','anomalies'])\nscaled_r_df=scaled_r_df.drop(columns=['Patient Event','EtCO2','anomalies'])","metadata":{"execution":{"iopub.status.busy":"2024-11-16T02:39:30.839365Z","iopub.execute_input":"2024-11-16T02:39:30.840292Z","iopub.status.idle":"2024-11-16T02:39:31.223116Z","shell.execute_reply.started":"2024-11-16T02:39:30.840257Z","shell.execute_reply":"2024-11-16T02:39:31.222137Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Valence Inversion Factor","metadata":{}},{"cell_type":"code","source":"# Assuming 'df' is your DataFrame\nimport pandas as pd\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\n# Calculate VIF for each feature in the DataFrame 'df'\nvif_data = pd.DataFrame()\nvif_data[\"Feature\"] = scaled_r_df.columns\nvif_data[\"VIF\"] = [variance_inflation_factor(scaled_r_df.values, i) for i in range(len(scaled_r_df.columns))]\n\nprint(vif_data)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-06T05:28:11.652985Z","iopub.execute_input":"2024-10-06T05:28:11.653874Z","iopub.status.idle":"2024-10-06T05:29:41.639059Z","shell.execute_reply.started":"2024-10-06T05:28:11.653837Z","shell.execute_reply":"2024-10-06T05:29:41.635318Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Pair-wise correlation","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Assuming 'df' is your DataFrame with features\ncorrelation_matrix = scaled_r_df.corr()\n\n# Display the correlation matrix\nprint(correlation_matrix)\n\n# Optional: To visualize the correlation matrix using a heatmap\nplt.figure(figsize=(12, 8))\nsns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\nplt.title('Pairwise Correlation Matrix')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-10-11T08:17:02.46174Z","iopub.execute_input":"2024-10-11T08:17:02.462111Z","iopub.status.idle":"2024-10-11T08:17:06.715392Z","shell.execute_reply.started":"2024-10-11T08:17:02.462083Z","shell.execute_reply":"2024-10-11T08:17:06.714481Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Condition number","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\n# Assuming 'df' is your DataFrame with features\nX = scaled_r_df.values  # Convert the DataFrame to a NumPy array\n\n# Calculate the condition number using the singular values\ncondition_number = np.linalg.cond(X)\n\nprint(f\"Condition Number: {condition_number}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-13T05:13:20.525712Z","iopub.execute_input":"2024-10-13T05:13:20.526123Z","iopub.status.idle":"2024-10-13T05:13:21.069873Z","shell.execute_reply.started":"2024-10-13T05:13:20.526092Z","shell.execute_reply":"2024-10-13T05:13:21.068862Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## PCA","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\n\n# Function to perform PCA and plot the explained variance ratio (elbow plot)\ndef plot_pca_explained_variance(df):\n    \"\"\"\n    This function performs PCA on the DataFrame and plots the cumulative explained variance \n    to help determine the optimal number of components using the elbow method.\n    \n    :param df: Input DataFrame.\n    :return: PCA model (sklearn PCA object) for further use.\n    \"\"\"\n    # Apply PCA with as many components as there are features\n    pca = PCA(n_components=min(df.shape[0], df.shape[1]))\n    pca.fit(df)\n    \n    # Calculate the cumulative explained variance\n    cumulative_explained_variance = np.cumsum(pca.explained_variance_ratio_)\n    \n    # Plot the elbow graph\n    plt.figure(figsize=(10, 6))\n    plt.plot(range(1, len(cumulative_explained_variance) + 1), cumulative_explained_variance, marker='o', linestyle='--')\n    plt.title('Explained Variance by Number of PCA Components')\n    plt.xlabel('Number of Components')\n    plt.ylabel('Cumulative Explained Variance')\n    plt.axhline(y=0.95, color='r', linestyle='--', label='95% Explained Variance')  # Optional 95% line\n    plt.legend()\n    plt.grid()\n    plt.show()\n    \n    return pca\n\n# Apply to your DataFrames (example: scaled_w_df)\n# pca_model_w = plot_pca_explained_variance(scaled_w_df)\n# pca_model_n1 = plot_pca_explained_variance(scaled_n1_df)\n# pca_model_n2 = plot_pca_explained_variance(scaled_n2_df)\n# pca_model_n3 = plot_pca_explained_variance(scaled_n3_df)\npca_model_r = plot_pca_explained_variance(scaled_r_df)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-13T05:13:24.528447Z","iopub.execute_input":"2024-10-13T05:13:24.528819Z","iopub.status.idle":"2024-10-13T05:13:26.068111Z","shell.execute_reply.started":"2024-10-13T05:13:24.528789Z","shell.execute_reply":"2024-10-13T05:13:26.067246Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Assuming you've used sklearn's PCA\nimport numpy as np\nimport pandas as pd\nfrom sklearn.decomposition import PCA\n\n# Let's assume 'data' is your dataset and 'pca' is the fitted PCA model\n# Perform PCA on your dataset\npca = PCA()\npca.fit(scaled_r_df)\n\n# Get the explained variance ratios\nexplained_variance_ratios = pca.explained_variance_ratio_\n\n# Get the cumulative explained variance\ncumulative_variance = np.cumsum(explained_variance_ratios)\n\n# Find out how many components are needed to explain 96% variance\nn_components_96 = np.argmax(cumulative_variance >= 0.96) + 1\nprint(f\"Number of components to reach 96% variance: {n_components_96}\")\n\n# Get the PCA components (loadings)\nloadings = pca.components_\n\n# Create a DataFrame with the features and their contributions to each principal component\n# Assuming 'feature_names' is the list of original feature names in your dataset\nloadings_df = pd.DataFrame(loadings[:n_components_96], columns=scaled_r_df.columns)\n\n# Print the contributions of features for the selected components\nprint(loadings_df)\n\n# Calculate the magnitude of the loadings\nloadings_df['magnitude'] = loadings_df.abs().max(axis=1)\n\n# Sort the loadings dataframe by magnitude in descending order\nsorted_loadings_df = loadings_df.sort_values(by='magnitude', ascending=False)\n\n# Drop the magnitude column if you want to remove it from the output\nsorted_loadings_df = sorted_loadings_df.drop('magnitude', axis=1)\n\n# Print the sorted loadings dataframe\nfor i in sorted_loadings_df:\n    print(i)\n# print(sorted_loadings_df)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-13T05:13:49.08091Z","iopub.execute_input":"2024-10-13T05:13:49.081516Z","iopub.status.idle":"2024-10-13T05:13:50.109108Z","shell.execute_reply.started":"2024-10-13T05:13:49.081486Z","shell.execute_reply":"2024-10-13T05:13:50.108119Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\n# Set pandas to display all columns\npd.set_option('display.max_columns', None)\n\n# Calculate the magnitude of the loadings\nloadings_df['magnitude'] = loadings_df.abs().max(axis=1)\n\n# Sort the loadings dataframe by magnitude in descending order\nsorted_loadings_df = loadings_df.sort_values(by='magnitude', ascending=False)\n\n# Drop the magnitude column if you don't want to display it\nsorted_loadings_df = sorted_loadings_df.drop('magnitude', axis=1)\n\n# Print the sorted loadings dataframe\nprint(sorted_loadings_df)","metadata":{"execution":{"iopub.status.busy":"2024-10-13T05:13:59.399173Z","iopub.execute_input":"2024-10-13T05:13:59.399865Z","iopub.status.idle":"2024-10-13T05:13:59.423532Z","shell.execute_reply.started":"2024-10-13T05:13:59.399833Z","shell.execute_reply":"2024-10-13T05:13:59.422573Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sorted_loadings_df.columns,sorted_loadings_df.shape","metadata":{"execution":{"iopub.status.busy":"2024-10-13T05:14:48.648606Z","iopub.execute_input":"2024-10-13T05:14:48.649006Z","iopub.status.idle":"2024-10-13T05:14:48.655825Z","shell.execute_reply.started":"2024-10-13T05:14:48.648976Z","shell.execute_reply":"2024-10-13T05:14:48.654759Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# RFC (partially annotated dataset)","metadata":{}},{"cell_type":"code","source":"# Calculate the number of rows representing the first 20% of the DataFrame\npercent = 0.30\npercent_index = int(percent * len(wavelet_features_df))\nprint(percent_index)\n# Slice the first 20% of the DataFrame\nfirst_percent_df = wavelet_features_df.iloc[:percent_index]\n\n# Count the number of anomalies in the first x% of the DataFrame\nanomaly_count_first_percent = (first_percent_df['anomalies'] == 1).sum()\n\nprint(f\"Number of anomalies in the first {percent*100}% of the DataFrame:\", anomaly_count_first_percent)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Import required libraries\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import balanced_accuracy_score, classification_report\n\nsleep_stage_dfs = [scaled_w_df,scaled_n1_df,scaled_n2_df,scaled_n3_df,scaled_r_df]\n\nfor test_df in sleep_stage_dfs:\n    # n = min(n,len(non_anomalies))\n\n    X = test_df.drop(columns=['anomalies'])\n    y = test_df['anomalies']\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.99, random_state=42)\n    \n    '''\n    # Select every 20th row for training\n    train_indices = X.index[::20]  # Get every 20th row index\n    test_indices = X.index.difference(train_indices)  # Remaining indices for testing\n    \n    # Create train and test sets\n    X_train = X.loc[train_indices]\n    y_train = y.loc[train_indices]\n    X_test = X.loc[test_indices]\n    y_test = y.loc[test_indices]\n    '''\n    \n    # Get the row indices of X_train\n    train_indices = X_train.index\n    \n    # Print the row indices\n    # print(\"Row indices in X_train:\", train_indices.tolist())\n    \n    print(len(X_train),len(X_test),len(y_train),len(y_test))\n    print(\"TRAIN SET: \", (y_train == 1).sum(), (y_train == 0).sum())\n    print(\"TEST SET: \", (y_test == 1).sum(), (y_test == 0).sum())\n\n    # Initialize the Random Forest Classifier\n    rfc = RandomForestClassifier(random_state=42)\n\n    # Train the model\n    rfc.fit(X_train, y_train)\n    # rfc.fit(X_train_balanced, y_train_balanced)\n\n    # Predict on the test set\n    y_pred = rfc.predict(X_test)\n\n    # Calculate balanced accuracy\n    balanced_acc = balanced_accuracy_score(y_test, y_pred)\n    acc_list.append(balanced_acc)\n    print(f\"Balanced Accuracy for n={n}: {balanced_acc:.4f}\")\n\n    # Optional: Classification report\n    print(f\"Classification Report for n={n}:\\n\", classification_report(y_test, y_pred))\n    print(\"=\"*50)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}